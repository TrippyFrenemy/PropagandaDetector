import csv
import re

import emoji
from tqdm import tqdm


class PreprocessorUA:
    def __init__(self, nlp):
        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª—ñ–≤
        with open('../data_manipulating/stopwords_ua.txt', 'r', encoding='utf-8') as f:
            self.__stop_words = set(line.strip().lower() for line in f if line.strip())
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–¥–µ–ª—ñ spaCy
        self.nlp = nlp

    # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤—á–æ—ó –æ–±—Ä–æ–±–∫–∏ —Ç–µ–∫—Å—Ç—É —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
    def __preprocess_text(self, text, lemma):
        if lemma:
            # –û–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º spaCy
            doc = self.nlp(text)
            # –õ–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è —Ç–æ–∫–µ–Ω—ñ–≤ —Ç–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —ñ–º–µ–Ω–æ–≤–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π
            lemmatized_tokens = [token.lemma_ for token in doc if not token.ent_type_]
            # –ó–±–∏—Ä–∞–Ω–Ω—è —Ä–µ—á–µ–Ω–Ω—è –Ω–∞–∑–∞–¥ –∑ —Ç–æ–∫–µ–Ω—ñ–≤
            text = ' '.join(lemmatized_tokens)
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–Ω—è –¥–æ –Ω–∏–∂–Ω—å–æ–≥–æ —Ä–µ–≥—ñ—Å—Ç—Ä—É
        text = text.lower()
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤ —Ç–∞ —Ü–∏—Ñ—Ä
        text = re.sub(r'\W', ' ', text)
        text = re.sub(r'\d', ' ', text)
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –æ–¥–∏–Ω–æ—á–Ω–∏—Ö –ª—ñ—Ç–µ—Ä
        text = re.sub(r'\s+[\u0400-\u04FF]\s+', ' ', text)
        text = text.replace("\n", " ").replace("\t", " ")
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –ø–æ—á–∞—Ç–∫–æ–≤–∏—Ö —Ç–∞ –∫—ñ–Ω—Ü–µ–≤–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
        text = text.strip()
        # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è (–ø–æ–¥—ñ–ª –Ω–∞ —Å–ª–æ–≤–∞)
        tokens = text.split()
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–æ–ø-—Å–ª—ñ–≤
        tokens = [word for word in tokens if word not in self.__stop_words]
        # –ó–±–∏—Ä–∞–Ω–Ω—è —Ä–µ—á–µ–Ω–Ω—è –Ω–∞–∑–∞–¥ –∑ —Ç–æ–∫–µ–Ω—ñ–≤
        text = ' '.join(tokens)
        return text

    def preprocess_corpus(self, corpus, lemma=True):
        processed_corpus = [self.__preprocess_text(text, lemma) for text in tqdm(corpus)]
        return processed_corpus


class EnhancedPreprocessorUA(PreprocessorUA):
    def __init__(self, nlp):
        super().__init__(nlp)
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–Ω–∏–∫–∞ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ
        self.__sentiment_dict = {}
        # self.__nlp = spacy.load('uk_core_news_sm')
        with open('../data_manipulating/sentiment_ua.csv', 'r', encoding='utf-8-sig') as f:
            reader = csv.reader(f, delimiter=';')
            next(reader, None)  # –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            for word, value in reader:
                self.__sentiment_dict[word.lower()] = int(value)

    def __extract_emotional_markers(self, text):
        """
        –í–∏—Ç—è–≥—É—î —Ç–∞ –∞–Ω–∞–ª—ñ–∑—É—î –µ–º–æ—Ü—ñ–π–Ω—ñ –º–∞—Ä–∫–µ—Ä–∏ –∑ —Ç–µ–∫—Å—Ç—É (–µ–º–æ–¥–∑—ñ —Ç–∞ —Ä–æ–∑–¥—ñ–ª–æ–≤—ñ –∑–Ω–∞–∫–∏).
        –ü–æ–≤–µ—Ä—Ç–∞—î –æ—á–∏—â–µ–Ω–∏–π –≤—ñ–¥ –µ–º–æ–¥–∑—ñ —Ç–µ–∫—Å—Ç —Ç–∞ —Å–ª–æ–≤–Ω–∏–∫ –µ–º–æ—Ü—ñ–π–Ω–∏—Ö –æ–∑–Ω–∞–∫.
        """
        emojis_list = [c for c in text if c in emoji.EMOJI_DATA]
        emoji_descriptions = [emoji.demojize(e) for e in emojis_list]
        exclamation_count = text.count('!')
        question_count = text.count('?')
        emotional_features = {
            'emoji_count': len(emojis_list),
            'emoji_descriptions': emoji_descriptions,
            'exclamation_count': exclamation_count,
            'question_count': question_count
        }
        cleaned_text = ''.join(c for c in text if c not in emoji.EMOJI_DATA)
        return cleaned_text, emotional_features

    def __analyze_syntax(self, doc):
        """
        –ê–Ω–∞–ª—ñ–∑—É—î —Å–∏–Ω—Ç–∞–∫—Å–∏—á–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é spaCy.
        –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ª–æ–≤–Ω–∏–∫ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–Ω–∏—Ö –æ–∑–Ω–∞–∫.
        """
        syntactic_features = {
            'sentence_count': len(list(doc.sents)),
            'dependency_patterns': [],
            'pos_tags': {}
        }
        for token in doc:
            if token.dep_:
                pattern = f'{token.head.text} --{token.dep_}--> {token.text}'
                syntactic_features['dependency_patterns'].append(pattern)
        for token in doc:
            pos = token.pos_
            syntactic_features['pos_tags'][pos] = syntactic_features['pos_tags'].get(pos, 0) + 1
        return syntactic_features

    def __extract_named_entities(self, doc):
        """
        –í–∏—Ç—è–≥—É—î —ñ–º–µ–Ω–æ–≤–∞–Ω—ñ —Å—É—Ç–Ω–æ—Å—Ç—ñ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é NER spaCy.
        –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ª–æ–≤–Ω–∏–∫ –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ —Å—É—Ç–Ω–æ—Å—Ç—ñ.
        """
        named_entities = {
            'entities': [],
            'entity_counts': {}
        }
        for ent in doc.ents:
            entity_info = {
                'text': ent.text,
                'label': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char
            }
            named_entities['entities'].append(entity_info)
            named_entities['entity_counts'][ent.label_] = named_entities['entity_counts'].get(ent.label_, 0) + 1
        return named_entities

    def __analyze_sentiment(self, text):
        """
        –í–∏–∫–æ–Ω—É—î –∞–Ω–∞–ª—ñ–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ —Ç–µ–∫—Å—Ç—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–Ω–∏–∫–∞.
        –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ª–æ–≤–Ω–∏–∫ –∑ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ (–ø–æ–ª—è—Ä–Ω—ñ—Å—Ç—å, —Å—É–±'—î–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —Ç–∞ —ñ–Ω.).
        """
        doc = self.nlp(text)
        words = [token.lemma_.lower() for token in doc if token.is_alpha]
        pos_count = sum(1 for w in words if w in self.__sentiment_dict and self.__sentiment_dict[w] > 0)
        neg_count = sum(1 for w in words if w in self.__sentiment_dict and self.__sentiment_dict[w] < 0)
        score = sum(self.__sentiment_dict[w] for w in words if w in self.__sentiment_dict)
        polarity = score / (pos_count + neg_count) if (pos_count + neg_count) > 0 else 0
        subjectivity = (pos_count + neg_count) / len(words) if len(words) > 0 else 0
        sentiment_features = {
            'polarity': polarity,
            'subjectivity': subjectivity,
            'positive_words': pos_count,
            'negative_words': neg_count
        }
        return sentiment_features

    def preprocess_text(self, text, include_features=True):
        """
        –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É –∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–º –≤–∏–¥—ñ–ª–µ–Ω–Ω—è–º –æ–∑–Ω–∞–∫.
        –ü–æ–≤–µ—Ä—Ç–∞—î (–æ–ø—Ä–∞—Ü—å–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç, —Å–ª–æ–≤–Ω–∏–∫ –æ–∑–Ω–∞–∫) —è–∫—â–æ include_features=True, —ñ–Ω–∞–∫—à–µ –æ–ø—Ä–∞—Ü—å–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç.
        """
        text, emotional_features = self.__extract_emotional_markers(text)
        doc = self.nlp(text)
        if include_features:
            features = {
                'emotional': emotional_features,
                'syntactic': self.__analyze_syntax(doc),
                'named_entities': self.__extract_named_entities(doc),
                'sentiment': self.__analyze_sentiment(text)
            }
        processed_text = super()._PreprocessorUA__preprocess_text(text, lemma=True)
        return (processed_text, features) if include_features else processed_text

    def preprocess_corpus(self, corpus, include_features=True):
        """
        –û–±—Ä–æ–±–ª—è—î –≤–µ—Å—å –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç—ñ–≤ –∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥–æ–º.
        –ü–æ–≤–µ—Ä—Ç–∞—î (—Å–ø–∏—Å–æ–∫ –æ–ø—Ä–∞—Ü—å–æ–≤–∞–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤, —Å–ø–∏—Å–æ–∫ –æ–∑–Ω–∞–∫) —è–∫—â–æ include_features=True, —ñ–Ω–∞–∫—à–µ —Å–ø–∏—Å–æ–∫ –æ–ø—Ä–∞—Ü—å–æ–≤–∞–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤.
        """
        results = [self.preprocess_text(text, include_features) for text in tqdm(corpus)]
        if include_features:
            processed_texts, features_list = zip(*results)
            return processed_texts, features_list
        else:
            return results


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    preprocessor = EnhancedPreprocessorUA()

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
    text = "–Ø –ª—é–±–ª—é —Ü–µ–π –ø—Ä–æ–¥—É–∫—Ç! üòä –î–∏—Ä–µ–∫—Ç–æ—Ä –Ü–≤–∞–Ω –ü–µ—Ç—Ä–µ–Ω–∫–æ –æ–≥–æ–ª–æ—Å–∏–≤ —á—É–¥–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏."
    processed_text, features = preprocessor.preprocess_text(text)
    print(processed_text)
    print(features)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–ø—É—Å–∞
    corpus = [
        "–ê–ø–µ–ª—å—Å–∏–Ω —î –ø–æ–º–∞—Ä–∞–Ω—á–µ–≤–∏–º.",
        "–ß–æ–º—É –º–∏ –ø–æ–≤–∏–Ω–Ω—ñ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –Ω–∞—à—É –≤–µ–ª–∏–∫—É –Ω–∞—Ü—ñ—é.",
        "–ù–∞—à–∞ –∫—Ä–∞—ó–Ω–∞ —Å—Ç–æ—ó—Ç—å –Ω–∞ –ø–æ—Ä–æ–∑—ñ –Ω–∞–π–±—ñ–ª—å—à–∏—Ö –¥–æ—Å—è–≥–Ω–µ–Ω—å –≤ —ñ—Å—Ç–æ—Ä—ñ—ó. –£ —Ç–æ–π —á–∞—Å —è–∫ –∑–ª–æ–≤–º–∏—Å–Ω—ñ –∫—Ä–∏—Ç–∏–∫–∏ –Ω–∞–º–∞–≥–∞—é—Ç—å—Å—è –ø—ñ–¥—ñ—Ä–≤–∞—Ç–∏ –Ω–∞—à –ø—Ä–æ–≥—Ä–µ—Å, –º–∏ –ø–æ–≤–∏–Ω–Ω—ñ –∑–∞–ª–∏—à–∞—Ç–∏—Å—è –æ–±'—î–¥–Ω–∞–Ω–∏–º–∏ —ñ –Ω–µ–ø–æ—Ö–∏—Ç–Ω–∏–º–∏.",
        "–ù–∞—à –ª—ñ–¥–µ—Ä - –Ω–∞–π–∫—Ä–∞—â–∏–π –∑ –Ω–∞–π–∫—Ä–∞—â–∏—Ö —É –≤—Å—å–æ–º—É —Å–≤—ñ—Ç—ñ.",
        "–©–æ–¥–µ–Ω–Ω–∞ –±–æ—Ä–æ—Ç—å–±–∞ –∑–≤–∏—á–∞–π–Ω–∏—Ö –ª—é–¥–µ–π –ø—Ä–æ—Ç–∏ –≤–ø–ª–∏–≤—É —Ñ–µ–º—ñ–Ω—ñ–∑–º—É."
    ]

    processed_texts, features_list = preprocessor.preprocess_corpus(corpus)
    print(processed_texts)
    print(features_list)
